当调用一个函数的时候，程序从函数头部开始执行，当函数退出
时，这个函数的声明周期也就结束了。一个函数在它的生命周期中
只能返回一次。
而协程在执行过程中，可以调用别的协程自己则途中退出执行，之后
又从调用别的协程的地方恢复运行。比较像线程，线程执行过程中
有可能被挂起，让位于别的线程执行，稍后又从挂起的地方恢复执行。
在这个过程中，协程与协程之间实际上不是普通“调用者与被调用者”的
关系，他们之间的关系是对称的。

libco是一种非对称的协程，即跟一个特定调用者绑定的，协程让出CPU，只能让回给原来的调用者。
非对称在于程序控制流移到被调用协成时使用的是call/resume操作。当被调用协程让出CPU时使用的却是
return/yield操作。此外，协程间的地位也不对等，caller和callee关系是确定的，不可更改的，非对称协程只能
返回最初调用它的协程。

对称协程则不一样，启动之后就跟启动之前的协程没有关系，协成的切换操作，一般而言只是一个操作yield，用于将程序控制
流转移给另外协程。对称协程机制一般需要一个调度器的支持，按一定的调度算法选择yield的目标协程。

libco中提供的协程，虽然编程接口跟pthread有点类似，，本质上却是一种非对称协程。

libco内部还为保存协程的调用链留了一个stack结构，而这个stack大小只有固定的128。
使用libco，如果不间断在一个协程中启动另一个协程，随着嵌套深度增加
就可能造成这个栈空间溢出。

```c
struct stTask_t {
    int id;
};

struct stEnv_t {
    stCoCond_t* cond;
    queue<stTask_t*> task_queue;
};

void* Producer(void* args) {
    co_enable_hook_sys();
    stEnv_t* env = (stEnv_t*)args;
    int id = 0;
    while (true) {
        stTask_t* task = (stTask_t*)calloc(1, sizeof(stTask_t));
        task->id = id++;
        env->task_queue.push(task);
        printf("%s:%d produce task %d\n", __func__, __LINE__, task->id);
        co_cond_signal(env->cond);
        poll(NULL, 0, 1000);
    }
    return NULL;
}

void* Consumer(void* args) {
    co_enable_hook_sys();
    stEnv_t* env = (stEnv_t*)args;
    while (true) {
        if (env->task_queue.empty()) {
            co_cond_timedwait(env->cond, -1);
            continue;
        }
        stTask_t* task = env->task_queue.front();
        env->task_queue.pop();
        printf("%s:%d consume task %d\n", __func__, __LINE__, task->id);
        free(task);
    }
    return NULL;
}

int main() {
    stEnv_t* env = new stEnv_t;
    env->cond = co_cond_alloc();
 
    stCoRoutine_t* consumer_routine;
    co_create(&consumer_routine, NULL, Consumer, env);
    co_resume(consumer_routine);
 
    stCoRoutine_t* producer_routine;
    co_create(&producer_routine, NULL, Producer, env);
    co_resume(producer_routine);
 
    co_eventloop(co_get_epoll_ct(), NULL, NULL);
    return 0;
}
```
co_enable_hook_sys()，此外，不是用sleep去等待，使用poll。

在main 函数中，，协程的结构叫stCoRoutine_t，创建一个协程使用co_create()函数，
使用形式和pthread_create相同

和pthread_create不太一样的是，创建一个协程之后，并没有立即启动起来，这里要启动协程，还需要调用co_resume函数，
最后，pthread创建线程之后主线程会有join等子线程退出，在这里的例子
没有co_join或类似的函数，而是调用了一个co_eventloop()函数。


libco的协程

线程的实现机制：
具备以下要素：
有一段程序供其执行，另外不同线程可以共用一段程序。

有私有财产，线程专属的堆栈空间。
有户口，操作系统维持的进程控制块。使得线程才能真正成为
内核调度的一个基本单位接受调度，这个结构记录着线程占有的各种资源。
操作系统的进程有自己专属的内存空间，不同进程间的内存空间
是相互独立的，互不干扰的。而同属一个进程的各个线程，共享进程的用户空间。协程也是共享内存空间的。

借鉴操作系统实现线程的思想，在OS之上实现用户级线程。跟OS线程一样，
用户线程也应该具备三要素：
不同的是第二点，用户级线程没有自己专属的对空间，只有栈空间。首先，我们得
准备一段程序供协程执行，这既是co_create函数在创建协程的时候传入的第三个参数，参数为void*，返回值为void*的一个函数

其次，需要为创建的协程准备一段栈内存空间。栈内存用于保存调用函数过程中的临时变量，以及函数调用链（栈帧）。
在intel的x86以及x64体系结构中，栈顶有esp寄存器确定，所以创建一个协成，启动的时候还要将esp切到分配的栈内存上。

co_create调用成功后，将返回一个stCoRoutine_t的结构指针。从命名上也可以看出来，该结构代表了libco的协程，记录着一个协程拥有的各种资源
，不妨称之为程控制块，这样协程三要素：执行函数，栈内存，协成控制块，在co_create()调用完成后，就准备就绪了。

关键数据结构和关系
libco的协程控制块stRoutine_t
```c
struct stCoRoutine_t{
       stCoRoutineEnv_t *env;
    pfn_co_routine_t pfn;
    void *arg;
    coctx_t ctx;
 
    char cStart;
    char cEnd;
    char cIsMain;
    char cEnableSysHook;
    char cIsShareStack;
 
    void *pvEnv;
 
    //char sRunStack[ 1024 * 128 ];
    stStackMem_t* stack_mem;
 
    //save satck buffer while confilct on same stack_buffer;
    char* stack_sp; 
    unsigned int save_size;
    char* save_buffer;
 
    stCoSpec_t aSpec[1024];
};
```
第2行env:libco的协程一旦创建后就跟创建时的那个线程绑定了的。是不支持在
不同线程间迁移的，这个env，即同属于一个县城所有协程的执行环境。
包括了当前运行协程、上次切换挂起的协程、嵌套调用的协程栈，和一个epoll的封装结构(TBD)
第3、４行，分别为实际待执行的协程函数以及参数
第5行，ctx是一个coctx_t类型的结构，用于协程切换的时候保存的CPU上下文的，所谓的上下文，就是esp、ebp、eip和其他通用寄存器的值
第7行至11行，是一些状态和标志变量，意义也很明了
pvEnv，名字看起来有点费解，我们暂时知道这是一个用于保存程序系统环境变量的指针就行。
第16行，stack_mem，协成运行时栈内存。固定大小128kb。

stackful协程的两种技术：
Separate coroutine stacks(独立的协程栈)和copying the stack(共享协程栈)
    实现细节：前者单独为每一个协程分配有一个单独的、固定大小的栈；而后者则仅为正在运行的协程分配栈内存
    ，当协程备切换出去的时候，就将他实际占用的占内存copy保存到一个单独分配的缓冲区；当被切出去的协程再次调度执行的时候，
    再一次copy将原来保存的栈内存恢复到那个共享的，固定大小的栈内存空间
通常情况下，一个协程实际占用的栈空间，相比与分配的这个大小要小的多，当然，协成切换时拷贝内存的开销有些场景下也是很大的。
libco实现了两种方案，默认使用独立协程栈，也允许用户创建协程时使用共享栈。

用于保存协程执行上下文的coctx_t结构
```
struct coctx_t {
#if defined(__i386__)
    void *regs[8];
#else
    void *regs[14];
#endif
    size_t ss_size;
    char *ss_sp;
};

```
协成控制块stCoRoutine_t结构中的第一个字段env，用于保存协成运行的“环境”
这个结构跟运行的线程绑定了，运行在同一个线程上的各协程是共享该结构的，
是个全局性的资源：
协程的stCoRoutineEnv_t
```c
struct stCoRoutineEnv_t {
    stCoRoutine_t *pCallStack[128];
    int iCallStackSize;
    stCoEpoll_t *pEpoll;

    // for copy stack log lastco and nextco
    stCoRoutine_t* pending_co;
    stCoRoutine_t* ocupy_co;
};

```
我们看到。stCoRoutineEnv内部有一个叫做CallStack的“栈”，还有一个stCoPoll_t结构指针。
此外，还有两个stCoRoutine_t指针用于记录协程切换时占用共享栈和将要切换运行的协程。在不使用
共享栈模式时，pending_co和ocupy_co都是空指针，我们暂时忽略他们，等到分析共享栈的时候再说

stCoRoutineEnv_t结构中的pCallStack不是普通意义上我们讲的程序运行栈，那个esp寄存器指向的栈，是用来保留
程序运行过程中局部变量以及函数调用关系的,但是这个pCallStack又跟esp有些相似之处，如果将协程看成一种特殊的函数,那么
这个pCallBack就是保存这些函数的调用链的栈。

非对称协程最大的特点是协程间存在明确的调用关系；甚至在有些文献中，启动称为call，挂起称为return。非对称协程机制下的被调协程只能范湖一
调用者协程，调用关系不能乱，因此必须讲调用链保存下来，这就是pCallBack的作用。他就是协程的调用
栈。

每当启动协程的时候，将它的协程控制块stCoRoutine_t结构指针保存在pCallBack的栈顶，然后栈指针iCallStackSize加1，最后切换context到待启动协程，运行，当协程要让出(yield)CPU时，
将它的stCoRoutine_t从pCallBack弹出，栈指针iCallBackSize减１，然后切换context到当前栈顶的协程恢复执行，这就是压栈和弹栈的过程，我们在co_resume()函数和co_yield()函数中可以看到。

要是libco一个协程，加入第一个协程yield时，CPU控制权让给谁？

都一个协程，实际上是执行main函数的协程，是一个特殊的协程，这个协程也是主协程。他负责协调其他协程的调度执行(后文我们看到，还有网络IO以及定时事件驱动)，他自己则永远不会yield，不会主动让出CPU，不让出(yield)CPU，不等于说他一直霸占着CPU

我们知道CPU执行权转移有两种途径：一是通过yield让给调用者，二是通过resume启动其他协程运行。后文我们就可以看到，co_resume()与co_yield()都伴随着上下文切换，即将CPU控制流的转移。当看到在程序中第一次调用co_resume()时，CPU执行权就从主协程转移到了resume目标协程了。

主协程什么时候创建的呢？什么时候resume呢？
事实上，主协程是跟stCoRoutinEnv_t一起创建的，主协程也无需调用resume来启动，他就是程序本身，就是main函数，主协程是一个特殊的存在。可以认为他只是一个结构体而已。
在程序首次调用co_create()时，此函数内部会判断当前进程的stCoRoutinEnv_t结构是否已分配，如果未分配，就分配一个，同时分配一个stCoRoutine_t结构，并将pCallStack[0]指向
主协程。伺候如果用co_resume()启动协程，又会将resume的协成压入pCallStack栈。

main函数中程序最终调用了co_eventloop()该函数是一个基于epoll/kqueue的事件循环，负责调度其他协程运行，具体细节暂时略去，stCoRoutineEnv_t结构中的pEpoll就是在这里用就够了。


libco协程的生命周期
创建协程
```c
int co_create(stCoRoutine_t** co, const stCoRoutineAttr_t* attr, void* (*routine)(void*), void* arg);
```
同pthread_create函数一样，该函数有四个参数：

co:stCoRoutine_t** 类型的指针，输出参数，co_create内部会为新协程分配一个"协程控制块"，*co将指向这个分配的协成控制块

attr:stCoRoutineAttr_t*类型的指针，输入参数，用于指定要创建协程的属性，可为NULL。实际上仅有两个属性：栈大小,指向共享栈的指针(共享栈模式)。

routine:void*(*)(void*)类型的函数指针，指向协程的任务函数，即启动这个协程后要完成什么样的任务，routine类型为函数指针。

arg:void*类型指针，传递给任务函数的参数，类似于pthread传递给线程的函数

调用co_create将协程创建出来后，这时候他还没有启动，也就是说我们传递的routine函数还没有被调用，实质上，这个函数仅仅是分配并初始化
stCoRoutine_t结构体，任务函数指针、分配一段栈内存，以及分配和初始化coctx_t。这个栈要加引号，因为栈内存们无论是使用预先分配的共享栈，还是co_create内部单独分配的栈，其实都是调用malloc从进程的堆内存分配出来，对于协程而言就是栈，而对于底层进程来说，只不过是普通的堆内存而已。

启动协程

在调用co_create创建协程返回成功后，便可以调用co_resume函数将他启动了，该函数声明如下：

```c
void co_resume(stCoRoutine_t* co);
```
启动co指针指向的协程
值得注意的是，为甚么在函数不叫co_start而是co_resume()呢？libco的协程是非对称协程，协程在让出CPU后要恢复执行的时候，还是要再次调用一下co_resume这个函数的区启动协程运行的。在语义上来讲，co_start只有一次
而co_resume可以是暂停之后恢复启动，可以多次调用。
实际上，看早期关于协程的文献，讲到非对称协程，一般也都用reusme与yield这两个术语，协程要获得CPU执行权用resume，而让出CPU执行权用yield，这两个是不同的过程，因此这种机制被称为非对称协程

所以讲到resume一个协程，我们一定注意，这可能是第一次启动该协程，也可以是要准备重新运行挂起
的协程，我们可以认为在libco中协程只有两种状态，
就是running和pending。
当创建一个协程并调用resume之后便进入了running 状态，
之后协程可能通过yield让出CPU，
这就进入了pending状态，不
断在这两这个状态循环往复，直到协程退出。
(执行任务函数的routine返回)

进入co_resume函数后发生协程的上下文切换，协程的任务函数是立即会被执行的，而且这个执行过程不是并发的。

为什么不是并发的呢？
因为co_resume函数内部会调用coctx_swap将当前协程挂起，然后就开始执行目标协程代码了，本质上这个过程是串行的，在一个操作系统线程上发生的，
甚至可以说在一颗CPU核上发生的。

将coroutine当做一种特殊的subroutine来看，问题会显得更清楚：
A协程调用co_resume(B)启动了协程。本质上是一种特殊的过程调用关系，
A调用B进入了B过程内部，这显然是一种串行执行的关系，那么co_resume调用后进入了被调用协成执行控制流
那么co_resume函数本身何时返回？这就要等被调协程主动让出CPU了

```c
void co_resume(stCoRoutine_t *co) {
    stCoRoutineEnv_t *env = co->env;
    stCoRoutine_t *lpCurrRoutine = env->pCallStack[env->iCallStackSize-1];
    if (!co->cStart) {
        coctx_make(&co->ctx, (coctx_pfn_t)CoRoutineFunc, co, 0);
        co->cStart = 1;
    }
    env->pCallStack[env->iCallStackSize++] = co;
    co_swap(lpCurrRoutine, co);
}
```
如果读者co_resume的逻辑还有疑问，不防再看一下它的代码实现。

以上第五、六行的if条件分之，当且仅当协程是第一次启动时才会执行到，首次启动协程过程有点特殊，
需要调用cotx_make为新准备的context(为了让co_swap内能跳转协程的任务函数)，并将cStart设置成1。
忽略第4-7行的特殊逻辑，那么co_resume仅有4行代码而已，第三行取当前协程控制块指针，第8行将待启动的
协程压入pCallStack栈，然后第9行就调用so_swap切换到co指向
的新协程上执行就行了，前文已经提到了，co_swap不会就此返回，而是要这次resume的co协程主动
yield让出CPU时才会返回到co_resume中来

值得指出的是，这里co_swap不会就此返回，不是说这个函数就阻塞在这里等待co这个协程，yield让出CPU，实际上我们将会看到so_swap内部已经切换了CPU执行上下文，
奔着co协程的代码路径去执行了。整个过程不是并发的，而是穿行的。

协程的挂起

在非对称协程理论，yield与resume 是一个相对的操作，A协程resume启动了B协程，那么只有当B协程执行yield操作时才会返回到A协成。在上一节剖析协程启动函数co_resume时，也提到了
该函数内部co_swap会执行被调用协成的代码，只有协程yield让出CPU，调用者协程的co_swap函数才能返回原点，即返回到原来co_resume内的位置

在前文解释stCoRoutineEnv_t结构pCallBack这个调用栈的时候，我们已经简要地提到了yield操作的内部逻辑，在被调用的协程让出CPU时，将它的stRoutine_t从pCallBack弹出，栈指针iCallStackSize减1，然后co_swap切换CPU上下文到原来被挂起的调用者协程恢复执行，这里，被挂起的调用者协程，就是调用者co_resume中切换CPU上下文别挂起的那个协程。下面来看co_yield_env函数代码：

```c
void co_yield_env(stCoRoutineEnv_t *env) {
    stCoRoutine_t *last = env->pCallStack[env->iCallStackSize - 2];
    stCoRoutine_t *curr = env->pCallStack[env->iCallStackSize - 1];
    env->iCallStackSize--;
    co_swap(curr, last);
}
```
co_yield_env函数仅有4行代码。
注意到这个函数为甚么叫co_yield_env而不是co_yield呢？
这个也很简单，我们知道co_resume是有明确目的对象的，而且可以通过resume讲将CPU交给人以协程。
但是yield则不一样，只能yield给当前协程的调用者，而当前协程的调用者，即最初resume当前协程的协程，是保存在stCoRoutineEnv_t的pCallBack中的，因此你只能yield给env，yield给调用者协程，而不能随意yield
给任意协程。

事实上libco提供了一个co_yield的函数，看起来可以给任意协程：
co_yield()函数
```
void co_yield(stCoRoutine_t *co) {
    co_yield_env(co->env);
}

```
同一个线程上所有协程是共享一个stCoRoutineEnv_t结构的，因此任意协程的co->env指向的结构都相同。

如果你调用co_yield(co)，会忽略传进来的参数。
libco的协程是不支持线程间迁移的。

协程的切换
ci_yield_env与co_resume，是两个完全相反的过程，但是任务都是一样的切换CPU执行的上下文，完成协成的切换。在co_resume中这个切换是从当前协程切换到被调用协程，在co_yield_env中，则是从当前协程切换到调用者协程，最终的上下文切换，都发生在co_swap函数内

严格来讲这里不是协成生命周期的一部分，而只是两个协成开始执行的与让出CPU时的一个临界点，既然是切换，
就涉及到两个协程，为了表达方便，我们准备将当前准备让出CPU的协程叫做current协程，将即将被调入执行的叫做pending协程。

coctx_swap.S汇编代码
```c

.globl coctx_swap
#if !defined( __APPLE__ )
.type  coctx_swap, @function
#endif
coctx_swap:

#if defined(__i386__)
    leal 4(%esp), %eax //sp
    movl 4(%esp), %esp
    leal 32(%esp), %esp //parm a : ®s[7] + sizeof(void*)

    pushl %eax //esp ->parm a
    pushl %ebp
    pushl %esi
    pushl %edi
    pushl %edx
    pushl %ecx
    pushl %ebx
    pushl -4(%eax)

    movl 4(%eax), %esp //parm b -> ®s[0]

    popl %eax  //ret func addr
    popl %ebx
    popl %ecx
    popl %edx
    popl %edi
    popl %esi
    popl %ebp
    popl %esp
    pushl %eax //set ret func addr

    xorl %eax, %eax
    ret
#elif defined(__x86_64__)
```
将其当做一个普通函数来看：
```c
void coctx_swap(coctx_t* curr, coctx_t* pending) asm("coctx_swap");

```
coctx_swap 接受两个参数，无返回值。其中，第一个参数 curr 为当前协程的 coctx_t 结构指针，其实是个输出参数，函数调用过程中会将当前协程的 context 保存在这个参数指向的内存里；第二个参数 pending，即待切入的协程的 coctx_t 指针，是个输入参数， coctx_swap 从这里取上次保存的 context，恢复各寄存器的值。

前面我们讲过 coctx_t 结构，就是用于保存各寄存器值（context）的。这个函数奇特之处，在于调用之前还处于第一个协程的环境，该函数返回后，则当前运行的协程就已经完全是第二个协程了。这跟 Linux 内核调度器的 switch_to 功能是非常相似的，只不过内核里线程的切换比这还要复杂得多。正所谓“杨百万进去，杨白劳出来”，当然，这里也可能是“杨白劳进去，杨百万出来”。

言归正题，这个函数既然是要直接操作寄存器，那当然非汇编不可了。汇编语言都快忘光了？那也不要紧，这里用到的都是常用的指令。值得一提的是，绝大多数学校汇编教程使用的都是 Intel 的语法格式，而这里用到的是 AT\&T 格式。

这里我们只需知道两者的主要差别，在于操作数的顺序是反过来的，就足够了。在 AT\&T 汇编指令里，如果指令有两个操作数，那么第一个是源操作数，第二个即为目的操作数。

此外，我们前面提到，这是个 C 风格的函数。什么意思呢？在 x86 平台下，多数 C 编译器会使用一种固定的方法来处理函数的参数与返回值。函数的参数使用栈传递，且约定了参数顺序，如图 3 所示。

在调用函数之前，编译器会将函数参数以反向顺序压栈，如图 3 中函数有三个参数，那么参数 3 首先 push 进栈，随后是参数 2，最后参数 1。准备好参数后，调用 CALL 指令时 CPU 自动将 IP 寄存器（函数返回地址）push 进栈，因此在进入被调函数之后，便形成了如图 3 的栈格局。函数调用结束前，则使用 EAX 寄存器传递返回值（如果 32 位够用的话），64 位则使用 EDX:EAX，如果是浮点值则使用 FPU ST(0) 寄存器传递。

在复习过这些汇编语言知识后，我们再来看 coctx_swap 函数。

它有两个参数，那么进入函数体后，用 4(%esp) 便可以取到第一个参数（当前协程 context 指针），8(%esp) 可以取到第二个参数（待切入运行协程的 context 指针）。当前栈顶的内容，(%esp) 则保存了coctx_swap 的返回地址。

搞清楚栈数据布局是理解 coctx_swap 函数的关键，接下来分析 coctx_swap 的每条指令，都需要时刻明白当前的栈在哪里，栈内数据是怎样一个分布。我们把 coctx_swap 分为两部分，以第 21 行那条 MOVL 指令为界。第一部分是用于保存 current 协程的各个寄存器，第二部分则是恢复 pending 协程的寄存器。接下来我们逐行进行分析。

第 8 行：LEA 指令即 Load Effective Address 的缩写。这条指令把 4(%esp) 有效地址保存到 eax 寄存器，可以认为是将当前的栈顶地址保存下来（实际保存的地址比栈顶还要高 4 字节，为了方便我们就称之为栈顶）。为什么要保存栈指针呢，因为紧接着就要进行栈切换了。

第 9~10 行：看到第 9 行，回忆我们前面讲过的 C 函数参数在栈中的位置，此时 4(%esp) 内正是指向 current 协程 coctx_t 的指针，这里把它塞到 esp 寄存器。接下来第 10 行又将 coctx_t 指针指向的地址加上 32 个字节的内存位置加载到 esp 中。经过这么一倒腾，esp 寄存器实际上指向了当前协程 coctx_t 结构的 ss_size 成员位置，在它之下有个名为 regs 的数组，刚好是用来保存 8 个寄存器值的。注意这是第一次栈切换，不过是临时性的，目的只是方便接下来使用 push 指令保存各寄存器值。

第12行：eax 寄存器内容压栈。更准确的讲，是将 eax 寄存器保存到了 coctx_t->regs[7] 的位置。注意到在第 8 行 eax 寄存器已经保存了原栈顶的地址，所以这句实际上是将当前协程栈顶保存起来，以备下次调度回来时恢复栈地址。

第13～18行：保存各通用寄存器的值，到 coctx_t 结构的 regs1~regs[6] 的位置。

第 19 行：这一行又有点意思了。eax 的值，从第 8 行之后就变变过，那么 -4(%eax) 实际上是指向原来 coctx_swap 刚进来时的栈顶，我们讲过栈顶的值是 call 指令自动压入的函数返回地址。这句实际上就是将 coctx_swap 的返回地址给保存起来了，放在 coctx_t->regs[0] 的位置。

第 21 行：至此，current 协程的各重要寄存器都已保存完成了，开始可以放心地交班给 pending 协程了。接下来我们需要将 pending 协程调度起来运行，就需要为它恢复 context——恢复各通用寄存器的值以及栈指针。因此这一行将栈指针切到 pending 协程的 coctx_t 结构体开始，即 regs[0] 的位置，为恢复寄存器值做好了准备。

第 23 行：弹出 regs[0] 的值到 eax 寄存器。regs[0] 正该协程上次被切换出去时在第 19 行保存的值，即 coctx_swap 的返回地址。

第 24～29 行：从 regs1～regs[6] 恢复各寄存器的值（与之相应的是前面第 13~18 行的压栈操作）。

第 30 行：将 pending 协程上次切换出去时的栈指针恢复（与之对应的是第 12 行压栈操作）。请思考一下，栈内容已经完全恢复了吗？注意到第 8 行我们讲过，当时保存的“栈顶”比真正的栈顶差了一个 4 字节的偏移。而这 4 字节真正栈顶的内容，正是 coctx_swap 的返回地址。如果此时程序就执行 ret 指令返回，那程序就不知道会跑到哪去了。

第 31 行：为了程序能正确地返回原来的 coctx_swap 调用的地方，将 eax 内容（第 19 行保存至 regs[7]，第 23 行取出来到 eax）压栈。
第 33~34 行：清零 eax 寄存器，执行返回指令。
至此，对 32 位平台的 coctx_swap 分析就到此结束了。

程退出
这里讲的退出，有别于协程的挂起，是指协程的任务函数执行结束后发生的过程。更简单的说，就是协程任务函数内执行了 return 语句，结束了它的生命周期。这在某些场景是有用的。同协程挂起一样，协程退出时也应将 CPU 控制权交给它的调用者，这也是调用 co_yield_env() 函数来完成的。这个机制很简单，限于篇幅，具体代码就不贴出来了。

值得注意的是，我们调用 co_create()、co_resume() 启动协程执行一次性任务，当任务结束后要记得调用 co_free()或co_release() 销毁这个临时性的协程，否则将引起内存泄漏。

事件驱动和协程调度

协成的阻塞和线程的非阻塞

从协程的角度看，这些等待看起来是同步的，阻塞的，但是从底层线程的角度来看，
是非阻塞的
在pthread实现的消费者中，你可能用pthread_cond_timedwait函数去同步等待生产者的信号，在消费者中
你可能用poll或sleep函数去等待

从线程角度看，这些函数都会让当前线程阻塞；但从内核角度看，他本身并没有阻塞，内核可能要继续忙着调度别的线程的运行。那么这里协程也是一样的，从协程的角度看，它本身并没有阻塞
，内核可能要继续忙着调度别的线程运行，那么这里协程也是一样的道理，从协程的角度看，当前的程序阻塞了，
但从它底下的线程来看，自己可能正忙着执行别的协程函数。当consumer协程调用co_cond_timedwait函数阻塞后，线程可能已经将producer恢复执行，反之依然，那么这个负责协程调度的线程在那呢？它既是运行协程本身的这个线程。

主协程与协程的调度

libco都有一个主协程，即程序中首次调用co_create显式创建第一个协程的协程。当consumer或者producer阻塞后，CPU将yield给主协程此时主协程在干什么？
主协程在co_eventloop中忙活。这个co_eventloop即调度器的核心所在。

需要说明的是，这里讲的"调度器"，严格意义上算不上真正的调度器，只是为了表述的方便。libco的协程机制是非对称的，没有什么调度算法

在执行yield时，当前协程只能将控制权交给调用者协程，没有调度的余地。resume灵活性稍强一点，不过也还算不得调度，如果非要说有什么调度算法的话，就只能说是基于epoll/kqueue事件驱动的调度算法
调度器就是epoll/kqueue的事件循环

libco将epoll/kqueue这样的IO事件通知机制完全隐藏了起来，你只需要使用普通的库函数read、write等同步写数据就行。

那epoll在哪里呢？就在主协程co_eventloop中，协程的调度与事件驱动是紧紧联系起来的，因此与其说libco是一个协程库，还不如说是一个网络库。

stCoEpoll_t结构与定时器
在分析stCoRoutineEnv_t结构的时候，还有一个stCoEpoll_t类型的pEpoll指针成员没有讲到。这个结构是一个全局性的资源，被同一个线程上所有协程共享，从命名也看得出来，stCoEpoll_t是跟epoll的事件循环相关的。
stCoEpoll_t结构
```c

struct stCoEpoll_t {
    int iEpollFd;
    static const int _EPOLL_SIZE = 1024 * 10;
    struct stTimeout_t *pTimeout;
    struct stTimeoutItemLink_t *pstTimeoutList;
    struct stTimeoutItemLink_t *pstActiveList;
    co_epoll_res *result; 
};
```
iEpollFd: 显然是 epoll 实例的文件描述符。
_EPOLL_SIZE: 值为 10240 的整型常量。作为 epoll_wait() 系统调用的第三个参数，即一次 epoll_wait 最多返回的就绪事件个数。
Timeout: 类型为 stTimeout_t 的结构体指针。该结构实际上是一个时间轮（Timing wheel）定时器，只是命名比较怪，让人摸不着头脑。
stTimeoutList: 指向 stTimeoutItemLink_t 类型的结构体指针。该指针实际上是一个链表头。链表用于临时存放超时事件的 item。
pstActiveList: 指向 stTimeoutItemLink_t 类型的结构体指针。也是指向一个链表。该链表用于存放 epoll_wait 得到的就绪事件和定时器超时事件。
result: 对 epoll_wait() 第二个参数的封装，即一次 epoll_wait 得到的结果集。

注册定时事件，需要指定未来触发的时间，在到了触发的时间点后，我们会收到定时器的通知。

网络框架第一部分保存已经注册timer events的数据结构，第二部分是定时通知机制。保存已注册的timer events，一般选用红黑树。
另外一种是时间轮，libco就是用了这种结构，当然你也可以直接使用链表实现，只是时间复度不一样，定时任务很多时会成为性能瓶颈。
定时器的第二部分，高精度的定时通知，一般使用getitimer/setitimer这类接口，需要处理信号，是个比较麻烦的事情。使用epoll加时间轮的实现定时器的算法如下：

Step 1 [epoll_wait] 调用epoll_wait() 等待 I/O 就绪事件，最大等待时长设置为 1 毫秒（即e poll_wait() 的 第 4 个参数）。
Step 2 [处理 I/O 就绪事件] 循环处理 epoll_wait() 得到的 I/O 就绪文件描述符。
Step 3 [从时间轮取超时事件] 从时间轮取超时事件，放到 timeout 队列。
Step 4 [处理超时事件] 如果 Step 3 取到的超时事件不为空，那么循环处理 timeout 队列中的定时任务。否则跳转到 Step 1 继续事件循环。
Step 5 [继续循环] 跳转到 Step 1，继续事件循环。

挂起协程和恢复的执行
协程在什么时候应该让出CPU呢？
libco中共有3种调用yield的场景：
用户程序主动调用co_yield_ct
程序调用了poll或者co_cond_timedwait陷入阻塞
程序调用了connect，read，write，recv，send等系统调用陷入阻塞等待。

相应的，重新resume启动一个协程共有三种情况：

对于用户程序主动yield的情况，这种情况也有赖于用户程序主动将协程co_resume起来
poll的目标文件描述符事件就绪或超时，co_cond_timedwait等到了其他协程的co_cond_signal通知信号等待超时

read，write等IO接口成功读到或者写入数据，或者读写超时

第一种情况下，及用户主动yield和resume协程，相当于libco使用者承担了部分的协程调度工作，这种情况其实很常见，
第二种情况，在前面的生产者消费者就是一个典型的例子，在那个例子中，看不到用户程序主动调用yield，也只有在最初启动协程时调用了resume。
生产者和消费者协程在哪里切换？在poll和co_cond_timewait函数中
当消费者协程启动的时候，他会发现队列任务是空的，于是调用co_cond_timedwait在条件变量cond上阻塞等待。同操作系统线程等待原理一样，
这里条件变量stCoCond_t类型也有一个等待队列，co_cond_timedwait函数内部会将当前协程挂入条件变量的等待队列上，并设置一个回调函数，该回调函数会用来唤醒当前协程的(即resume挂起的协程)
   
此外如果wait的timeout参数大于0，还要向当前执行环境的定时器上注册一个定时事件。在这个例子中，消费者协程co_cond_timewait的timeout参数为-1，即identifinity地等待下去，直到接收到signal信号。

生产者，当生产者协程启动后，他会向任务队列中投放一个任务并调用co_cond_signal通知消费者，然后调用poll在原地阻塞1000毫秒，这里co_cond_signal函数内部，将条件变量的等待队列中的协程拿出，然后挂到当前执行环境的pstActiveList。co_cond_signal函数并没有立即resume条件变量上的等待协程。
什么时候交出CPU控制权呢，什么时候resume消费者协程呢？
生产者在向消费者发出信号后，首先将自己作为定时时间注册到当前执行环境的定时器中，注册的时候设置了一秒钟的超时时间和一个回调函数仍是一个用于未来唤醒自己的回调，然后就调用co_yield_env将cpu让给主协程
拿到CPU控制权，主协程干什么？
执行co_eventloop函数，在该函数中，主协程周而复始调用epoll_wait，当有就绪的IO事件就处理IO事件，当定时器上有超时事件就处理超时事件，pstActieveList队列中有活跃事件就处理活跃事件，这里所谓的处理事件就是调用其他协程注册的各种回调函数。

消费者协成和生产者协程的回调函数就是唤醒自己而已，工作协程调用co_cond_timedwait或者poll陷入在阻塞。本质上就是通过co_yield_env函数让出了协程注册的回调函数，这些回调函数内部使用了co_resume重新恢复官气的工作线程。

最后协程yield和resume的三种情况，就是调用了read，write等IO操作而陷入了阻塞和最后又恢复执行的过程，我们知道libco的协程是在底层线程上串行执行的。

如果调用了read和write等系统调用陷入真正的阻塞的话，那么不光当前协程挂起了，其他协程也得不到执行的机会，因此，如果工作协成陷入真正的内核态这颜色，那么libco程序就会完全停止运转，后果很严重的。
为了避免陷入内核态阻塞，我们必须得依靠内核提供的非阻塞IO基质，将socket文件描述符设置成no-blcoking，为了让libco的使用更方便，我们还得将这种non-blcoking的过程封装，伪装成同步阻塞调用

事实上，GO语言就是这么干的，而libco将这个过程封装的更加彻底，通过dlsym机制hook了各种网络相关的系统调用，使得用户可以以的同步的方式直接使用read,write,connect

因此，我们看到co_enable_hook_sys函数，调用了co_enable_hook_sys才会开启hook系统调用功能，并且需要首先将要读写的文件描述符设置成non-blocking属性，否则，工作协程就可能陷入到内核态阻塞。

libco内部的read函数：
这函数内部实际上做了4件事，第一步将当前协成注册到定时器上，用于将来处理read函数读超时。
第二步，调用epoll_ctl将自己注册到当前执行环境的epoll中，这两步注册过程都需要指定一个回调函数，将来用于唤醒当前协程，第三部，调用co_yield_env函数让出CPU，第四步，等到协程被主协程重新唤醒后才能继续。

如果主协程epoll_wait得知read操作的文件描述符可读，则会执行原来read协程注册的回调将它唤醒，工作协程被唤醒后，调用原来Glibc中被hook替换掉的真正的read系统调用。这时候如果是正常epoll_wait得知文件描述符有IO就绪，就会读到数据，如果超时，返回-1。

在外部看来这个read和阻塞式系统调用表现出相同的行为。

```c
void co_eventloop(stCoEpoll_t *ctx, pfn_co_eventloop_t pfn, void *arg)
{
    co_epoll_res *result = ctx->result;
 
    for (;;) {
        ////等待IO就绪事件，为了配合时间轮工作，这里的timeout设置为1ms
        int ret= co_epoll_wait(ctx->iEpollFd, result, stCoEpoll_t::_EPOLL_SIZE, 1);
        //指向当前活跃的事件队列
        stTimeoutItemLink_t *active = (ctx->pstActiveList);
        //临时性的链表
        stTimeoutItemLink_t *timeout = (ctx->pstTimeoutList);
        memset(timeout, 0, sizeof(stTimeoutItemLink_t));
        for (int i=0; i<ret; i++) {
            stTimeoutItem_t *item = (stTimeoutItem_t*)result->events[i].data.ptr;
            //做预处理
            if (item->pfnPrepare) {
                //将事件加到active队列
                item->pfnPrepare(item, result->events[i], active);
            } else {
                AddTail(active, item);
            }
        }
 
        unsigned long long now = GetTickMS();
        //取出已经超时的事件
        TakeAllTimeout(ctx->pTimeout, now, timeout);
        //遍历timeout队列，设置事件已超时的标志
        stTimeoutItem_t *lp = timeout->head;
        while (lp) {
            lp->bTimeout = true;
            lp = lp->pNext;
        }
        //将timeout队列中的事件与active事件合并
        Join<stTimeoutItem_t, stTimeoutItemLink_t>(active, timeout);
        //遍历active队列，调用工作鞋城设置的pfnProcess灰调函数resume挂起的工作协成
        //，处理对应的IO或超时事件
        lp = active->head;
        while (lp) {
            PopHead<stTimeoutItem_t, stTimeoutItemLink_t>(active);
            if (lp->pfnProcess) {
                lp->pfnProcess(lp);
            }
            lp = active->head;
        }
    }
}
 
```

这就是主协程的事件循环工作过程，我们看到它周而复始地 epoll_wait()，
唤醒挂起的工作协程去处理定时器与 I/O 事件。这里的逻辑看起来跟所有基于 
epoll 实现的事件驱动网络框架并没有什么特别之处，更没有涉及到任何协程调度算法，
由此也可以看到 libco 其实是一个很典型的非对称协程机制。
或许，从 call/return 的角度出发，
而不是 resume/yield 去理解这种协程的运行机理，反而会有更深的理解吧。
